
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Brain decoding with MLP &#8212; Introduction to brain decoding &amp; encoding in fMRI</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script src="_static/design-tabs.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="canonical" href="https://main-educational.github.io/brain_encoding_decoding/mlp_decoding.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Brain decoding with GCN" href="gcn_decoding.html" />
    <link rel="prev" title="Brain decoding with SVM" href="svm_decoding.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/neurolibre-logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to brain decoding & encoding in fMRI</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="haxby_data.html">
   An overview of the Haxby dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="svm_decoding.html">
   Brain decoding with SVM
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Brain decoding with MLP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gcn_decoding.html">
   Brain decoding with GCN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="encoding.html">
   Brain encoding
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="_sources/mlp_decoding.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/mlp_decoding.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/main-educational/brain_encoding_decoding"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/main-educational/brain_encoding_decoding/issues/new?title=Issue%20on%20page%20%2Fmlp_decoding.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/main-educational/brain_encoding_decoding/edit/main/content/mlp_decoding.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/main-educational/brain_encoding_decoding/main?urlpath=tree/content/mlp_decoding.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multilayer-perceptron">
   Multilayer Perceptron
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#getting-the-data">
   Getting the data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-a-model">
   Training a model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assessing-performance">
   Assessing performance
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Brain decoding with MLP</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multilayer-perceptron">
   Multilayer Perceptron
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#getting-the-data">
   Getting the data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-a-model">
   Training a model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assessing-performance">
   Assessing performance
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="brain-decoding-with-mlp">
<h1>Brain decoding with MLP<a class="headerlink" href="#brain-decoding-with-mlp" title="Permalink to this headline">¬∂</a></h1>
<p>This part of the <code class="docutils literal notranslate"><span class="pre">session</span></code> aims to make <code class="docutils literal notranslate"><span class="pre">participants</span></code> familiar with <a class="reference external" href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multilayer Peceptrons</a> as one possible <code class="docutils literal notranslate"><span class="pre">decoding</span> <span class="pre">model</span></code> that can be applied to <code class="docutils literal notranslate"><span class="pre">brain</span> <span class="pre">data</span></code>. The objectives üìç are:</p>
<ul class="simple">
<li><p>get to know the basics of <code class="docutils literal notranslate"><span class="pre">Multilayer</span> <span class="pre">Peceptrons</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code> creation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code> <code class="docutils literal notranslate"><span class="pre">training</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code> <code class="docutils literal notranslate"><span class="pre">testing</span></code></p></li>
</ul>
</li>
</ul>
<div class="section" id="multilayer-perceptron">
<h2>Multilayer Perceptron<a class="headerlink" href="#multilayer-perceptron" title="Permalink to this headline">¬∂</a></h2>
<div class="figure align-default" id="multilayer-perceptron-fig">
<a class="reference internal image-reference" href="_images/multilayer-perceptron.png"><img alt="_images/multilayer-perceptron.png" src="_images/multilayer-perceptron.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text">A multilayer perceptron with 25 units on the input layer, a single hidden layer with 17 units, and an output layer with 9 units. Figure generated with the <a class="reference external" href="http://alexlenail.me/NN-SVG/index.html">NN-SVG</a> tool by [Alexander Lenail]. The figure is shared under a <a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a> license.</span><a class="headerlink" href="#multilayer-perceptron-fig" title="Permalink to this image">¬∂</a></p>
</div>
<p>We are going to train a <code class="docutils literal notranslate"><span class="pre">Multilayer</span> <span class="pre">Perceptron</span></code> (<code class="docutils literal notranslate"><span class="pre">MLP</span></code>) <code class="docutils literal notranslate"><span class="pre">classifier</span></code> for <code class="docutils literal notranslate"><span class="pre">brain</span> <span class="pre">decoding</span></code> on the <a class="reference external" href="https://main-educational.github.io/brain_encoding_decoding/haxby_data.html">Haxby dataset</a>. <code class="docutils literal notranslate"><span class="pre">MLP</span></code>s are one of the most basic architecture of <a class="reference external" href="https://en.wikipedia.org/wiki/Artificial_neural_network">artificial neural networks</a>. As such, <code class="docutils literal notranslate"><span class="pre">MLP</span></code>s consist of <code class="docutils literal notranslate"><span class="pre">input</span></code> and <code class="docutils literal notranslate"><span class="pre">output</span></code> <code class="docutils literal notranslate"><span class="pre">layers</span></code> as well as <code class="docutils literal notranslate"><span class="pre">hidden</span> <span class="pre">layers</span></code> that process the <code class="docutils literal notranslate"><span class="pre">input</span></code> through a succession of <code class="docutils literal notranslate"><span class="pre">transformations</span></code> towards the <code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">layer</span></code> that performs the task at hand, e.g. a <code class="docutils literal notranslate"><span class="pre">classification</span></code> or <code class="docutils literal notranslate"><span class="pre">regression</span></code>. Like other <code class="docutils literal notranslate"><span class="pre">machine</span> <span class="pre">learning</span> <span class="pre">models</span></code> for <code class="docutils literal notranslate"><span class="pre">supervised</span> <span class="pre">learning</span></code>, an <code class="docutils literal notranslate"><span class="pre">MLP</span></code> initially goes through a <code class="docutils literal notranslate"><span class="pre">training</span> <span class="pre">phase</span></code>. During this <code class="docutils literal notranslate"><span class="pre">supervised</span> <span class="pre">phase</span></code>, the <code class="docutils literal notranslate"><span class="pre">network</span></code> is taught what to look for and what is the desired output via its <code class="docutils literal notranslate"><span class="pre">objective</span> <span class="pre">function</span></code>. This refers to, minimizing the <code class="docutils literal notranslate"><span class="pre">loss</span></code>, ie the deviation of <code class="docutils literal notranslate"><span class="pre">predictions</span></code> from the ‚Äúground truth‚Äù, and thus increasing its performance.</p>
<p><code class="docutils literal notranslate"><span class="pre">MLP</span></code>s were actually among the first <code class="docutils literal notranslate"><span class="pre">ANN</span></code>s to appear, specifically the <a class="reference external" href="https://en.wikipedia.org/wiki/Perceptron">Mark I Peceptron</a> which you can see below.</p>
<div class="figure align-default" id="marki-perceptron-fig">
<a class="reference internal image-reference" href="https://preview.redd.it/wgzps0pvcny91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0b2e56dc4eaa886ebd01ac0cd8e51fc4efdb1d01"><img alt="https://preview.redd.it/wgzps0pvcny91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0b2e56dc4eaa886ebd01ac0cd8e51fc4efdb1d01" src="https://preview.redd.it/wgzps0pvcny91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0b2e56dc4eaa886ebd01ac0cd8e51fc4efdb1d01" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text">Frank Rosenblatt with a Mark I Perceptron computer in 1960.</span><a class="headerlink" href="#marki-perceptron-fig" title="Permalink to this image">¬∂</a></p>
</div>
<p>In this tutorial, we are going to train the simplest <code class="docutils literal notranslate"><span class="pre">MLP</span></code> architecture featuring one <code class="docutils literal notranslate"><span class="pre">input</span> <span class="pre">layer</span></code>, one <code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">layer</span></code> and just one <code class="docutils literal notranslate"><span class="pre">hidden</span> <span class="pre">layer</span></code>.</p>
</div>
<div class="section" id="getting-the-data">
<h2>Getting the data<a class="headerlink" href="#getting-the-data" title="Permalink to this headline">¬∂</a></h2>
<p>We are going to work with the Haxby dataset <span id="id1">[<a class="reference internal" href="haxby_data.html#id6">HGF+01</a>]</span> again. You can check the section <a class="reference internal" href="haxby_data.html#haxby-dataset"><span class="std std-ref">An overview of the Haxby dataset</span></a> for more details on that <code class="docutils literal notranslate"><span class="pre">dataset</span></code>. Here we are going to quickly <code class="docutils literal notranslate"><span class="pre">download</span></code> and prepare it for <code class="docutils literal notranslate"><span class="pre">machine</span> <span class="pre">learning</span> <span class="pre">applications</span></code> with a set of <code class="docutils literal notranslate"><span class="pre">predictive</span> <span class="pre">variables</span></code>, the <code class="docutils literal notranslate"><span class="pre">brain</span> <span class="pre">time</span> <span class="pre">series</span></code> <code class="docutils literal notranslate"><span class="pre">X</span></code>, and a <code class="docutils literal notranslate"><span class="pre">dependent</span> <span class="pre">variable</span></code>, the respective <code class="docutils literal notranslate"><span class="pre">cognitive</span> <span class="pre">processes</span></code>/<code class="docutils literal notranslate"><span class="pre">function</span></code>/<code class="docutils literal notranslate"><span class="pre">percepts</span></code> <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import os
import warnings
warnings.filterwarnings(action=&#39;once&#39;)

from nilearn import datasets
# We are fetching the data for subject 4
data_dir = os.path.join(&#39;..&#39;, &#39;data&#39;)
sub_no = 4
haxby_dataset = datasets.fetch_haxby(subjects=[sub_no], fetch_stimuli=True, data_dir=data_dir)
func_file = haxby_dataset.func[0]

# mask the data
from nilearn.input_data import NiftiMasker
mask_filename = haxby_dataset.mask_vt[0]
masker = NiftiMasker(mask_img=mask_filename, standardize=True, detrend=True)
X = masker.fit_transform(func_file)

# cognitive annotations
import pandas as pd
behavioral = pd.read_csv(haxby_dataset.session_target[0], delimiter=&#39; &#39;)
y = behavioral[&#39;labels&#39;]
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/nilearn/datasets/func.py:20: DeprecationWarning: Please use `MatReadError` from the `scipy.io.matlab` namespace, the `scipy.io.matlab.miobase` namespace is deprecated.
  from scipy.io.matlab.miobase import MatReadError
/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/nilearn/datasets/__init__.py:93: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.
  warn(&quot;Fetchers from the nilearn.datasets module will be &quot;
</pre></div>
</div>
</div>
</div>
<p>As an initial check, we‚Äôll have a look at the size of <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>categories = y.unique()
print(categories)
print(y.shape)
print(X.shape)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;rest&#39; &#39;face&#39; &#39;chair&#39; &#39;scissors&#39; &#39;shoe&#39; &#39;scrambledpix&#39; &#39;house&#39; &#39;cat&#39;
 &#39;bottle&#39;]
(1452,)
(1452, 675)
</pre></div>
</div>
</div>
</div>
<p>So we have <code class="docutils literal notranslate"><span class="pre">1452</span></code> <code class="docutils literal notranslate"><span class="pre">time</span> <span class="pre">points</span></code>, with one <code class="docutils literal notranslate"><span class="pre">label</span></code> for the respective <code class="docutils literal notranslate"><span class="pre">stimulus</span> <span class="pre">percept</span></code> each, and for each <code class="docutils literal notranslate"><span class="pre">time</span> <span class="pre">point</span></code> we have <code class="docutils literal notranslate"><span class="pre">recordings</span></code> of <code class="docutils literal notranslate"><span class="pre">brain</span></code> activity obtained via  <code class="docutils literal notranslate"><span class="pre">fMRI</span></code> across <code class="docutils literal notranslate"><span class="pre">675</span> <span class="pre">voxels</span></code> (within the <code class="docutils literal notranslate"><span class="pre">VT</span></code> <code class="docutils literal notranslate"><span class="pre">mask</span></code>). We can also see that the <code class="docutils literal notranslate"><span class="pre">stimulus</span> <span class="pre">percept</span></code>s span <code class="docutils literal notranslate"><span class="pre">9</span></code> different <code class="docutils literal notranslate"><span class="pre">categories</span></code>.</p>
<p>However, concerning our planned analyses, we need to convert our <code class="docutils literal notranslate"><span class="pre">categories</span></code> into a <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html">one-hot encoder</a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># creating instance of one-hot-encoder
from sklearn.preprocessing import OneHotEncoder
import numpy as np
enc = OneHotEncoder(handle_unknown=&#39;ignore&#39;)
y_onehot = enc.fit_transform(np.array(y).reshape(-1, 1))
# turn the sparse matrix into a pandas dataframe
y = pd.DataFrame(y_onehot.toarray())
display(y)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1447</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1448</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1449</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1450</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1451</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>1452 rows √ó 9 columns</p>
</div></div></div>
</div>
</div>
<div class="section" id="training-a-model">
<h2>Training a model<a class="headerlink" href="#training-a-model" title="Permalink to this headline">¬∂</a></h2>
<p>As introduced in the prior <code class="docutils literal notranslate"><span class="pre">tutorials</span></code>, one of the most important aspects of <code class="docutils literal notranslate"><span class="pre">machine</span> <span class="pre">learning</span></code> is the split between <code class="docutils literal notranslate"><span class="pre">train</span></code> and <code class="docutils literal notranslate"><span class="pre">tests</span></code>. <code class="docutils literal notranslate"><span class="pre">MLP</span></code>s are no exception to that and thus we need to split our dataset accordingly. We will keep <code class="docutils literal notranslate"><span class="pre">20%</span></code> of the <code class="docutils literal notranslate"><span class="pre">time</span> <span class="pre">points</span></code> as <code class="docutils literal notranslate"><span class="pre">test</span></code>, and then set up a <code class="docutils literal notranslate"><span class="pre">10</span> <span class="pre">fold</span> <span class="pre">cross</span> <span class="pre">validation</span></code> for <code class="docutils literal notranslate"><span class="pre">training/validation</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)   
</pre></div>
</div>
</div>
</div>
<p>With that, we can already build our <code class="docutils literal notranslate"><span class="pre">MLP</span></code>. Here, we are going to use <a class="reference external" href="https://www.tensorflow.org/">Tensorflow</a> and <a class="reference external" href="https://keras.io/">Keras</a>. As with every other <code class="docutils literal notranslate"><span class="pre">ANN</span></code>, we need to <code class="docutils literal notranslate"><span class="pre">import</span></code> the respective components, here, the <code class="docutils literal notranslate"><span class="pre">model</span></code> and <code class="docutils literal notranslate"><span class="pre">layer</span></code> <code class="docutils literal notranslate"><span class="pre">type</span></code>. In our case we will use a <a class="reference external" href="https://keras.io/guides/sequential_model/"><code class="docutils literal notranslate"><span class="pre">Sequential</span></code> <code class="docutils literal notranslate"><span class="pre">model</span></code></a> and <a class="reference external" href="https://keras.io/api/layers/core_layers/dense/"><code class="docutils literal notranslate"><span class="pre">Dense</span></code></a> <code class="docutils literal notranslate"><span class="pre">layers</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from keras.models import Sequential
from keras.layers import Dense
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-12-09 20:51:55.403353: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libcudart.so.11.0&#39;; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-12-09 20:51:55.403390: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  &#39;nearest&#39;: pil_image.NEAREST,
/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  &#39;bilinear&#39;: pil_image.BILINEAR,
/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  &#39;bicubic&#39;: pil_image.BICUBIC,
/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, &#39;HAMMING&#39;):
/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, &#39;BOX&#39;):
/opt/hostedtoolcache/Python/3.8.15/x64/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, &#39;LANCZOS&#39;):
</pre></div>
</div>
</div>
</div>
<div class="tip admonition">
<p class="admonition-title">A note regarding our MLP</p>
<p>Please note that the example <code class="docutils literal notranslate"><span class="pre">MLP</span></code> we are going to <code class="docutils literal notranslate"><span class="pre">create</span></code> and <code class="docutils literal notranslate"><span class="pre">train</span></code> here is rather simple as we want to enable its application on machines with rather limited computational resources (ie your laptops or binder). ‚ÄúReal-world‚Äù models are usually more complex and might also entail different <code class="docutils literal notranslate"><span class="pre">types</span></code> and <code class="docutils literal notranslate"><span class="pre">layers</span></code>.</p>
</div>
<p>Initially, we need to create our, so far, <code class="docutils literal notranslate"><span class="pre">empty</span> <span class="pre">model</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># number of unique conditions that we have
model_mlp = Sequential()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-12-09 20:51:56.916155: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libcuda.so.1&#39;; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2022-12-09 20:51:56.916188: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2022-12-09 20:51:56.916217: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fv-az167-841): /proc/driver/nvidia/version does not exist
2022-12-09 20:51:56.916517: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
</pre></div>
</div>
</div>
</div>
<p>Next, we can add the <code class="docutils literal notranslate"><span class="pre">layers</span></code> to our <code class="docutils literal notranslate"><span class="pre">model</span></code>, starting with the <code class="docutils literal notranslate"><span class="pre">input</span> <span class="pre">layer</span></code>. Given this is a rather short introduction to the topic and does not focus on <code class="docutils literal notranslate"><span class="pre">ANN</span></code>s, we are going to set the <code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">initialization</span></code> and <code class="docutils literal notranslate"><span class="pre">activation</span> <span class="pre">function</span></code> to appropriate defaults (Please have a look at the <a class="reference external" href="https://main-educational.github.io/material.html#introduction-to-deep-learning-using-pytorch">Introduction to deep learning session</a> for more information.).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model_mlp.add(Dense(50 , input_dim = 675, kernel_initializer=&quot;uniform&quot;, activation = &#39;relu&#39;))
</pre></div>
</div>
</div>
</div>
<p>As noted above, we are using <code class="docutils literal notranslate"><span class="pre">Dense</span></code> <code class="docutils literal notranslate"><span class="pre">layers</span></code> and as you can see, we set the <code class="docutils literal notranslate"><span class="pre">input</span> <span class="pre">dimensions</span></code> to <code class="docutils literal notranslate"><span class="pre">675</span></code>. You might have already notices that this is the number of <code class="docutils literal notranslate"><span class="pre">voxels</span></code> we have <code class="docutils literal notranslate"><span class="pre">data</span></code> from. Setting the <code class="docutils literal notranslate"><span class="pre">input</span> <span class="pre">dimension</span></code> according to the <code class="docutils literal notranslate"><span class="pre">data</span> <span class="pre">dimensions</span></code> is rather important is referred to as the <a class="reference external" href="https://en.wikipedia.org/wiki/Semantic_gap">semantic gap</a>: the transformation of <code class="docutils literal notranslate"><span class="pre">actions</span></code> &amp; <code class="docutils literal notranslate"><span class="pre">percepts</span></code> conducted/perceived by <code class="docutils literal notranslate"><span class="pre">human</span></code>s into <code class="docutils literal notranslate"><span class="pre">computational</span> <span class="pre">representations</span></code>. For example, pictures are ‚Äúnothing‚Äù but a huge <code class="docutils literal notranslate"><span class="pre">array</span></code> for a computer and what will be submitted to the input layer of an <code class="docutils literal notranslate"><span class="pre">ANN</span></code> (note: this also holds true for basically any other type of <code class="docutils literal notranslate"><span class="pre">data</span></code>). Here, our <code class="docutils literal notranslate"><span class="pre">MLP</span></code> receives the extracted <code class="docutils literal notranslate"><span class="pre">brain</span> <span class="pre">activity</span> <span class="pre">patterns</span></code> as <code class="docutils literal notranslate"><span class="pre">input</span></code> which are already in the right <code class="docutils literal notranslate"><span class="pre">array</span></code> format thanks to <code class="docutils literal notranslate"><span class="pre">nilearn</span></code>. Thus, always carefully think about what your <code class="docutils literal notranslate"><span class="pre">input</span></code> <code class="docutils literal notranslate"><span class="pre">data</span></code> entails and how it is structured to then setup your <code class="docutils literal notranslate"><span class="pre">input</span> <span class="pre">layer</span></code> accordingly.</p>
<p>Next, we are going to add one <code class="docutils literal notranslate"><span class="pre">hidden</span> <span class="pre">layer</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model_mlp.add(Dense(30, kernel_initializer=&quot;uniform&quot;, activation = &#39;relu&#39;))
</pre></div>
</div>
</div>
</div>
<p>And because we are creating a very simple <code class="docutils literal notranslate"><span class="pre">MLP</span></code> with only three <code class="docutils literal notranslate"><span class="pre">layers</span></code>, we already add our <code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">layer</span></code>, using the <code class="docutils literal notranslate"><span class="pre">softmax</span></code> <code class="docutils literal notranslate"><span class="pre">activation</span> <span class="pre">function</span></code> given that we aim to <code class="docutils literal notranslate"><span class="pre">train</span></code> our <code class="docutils literal notranslate"><span class="pre">MLP</span></code> to <code class="docutils literal notranslate"><span class="pre">predict</span></code> the different <code class="docutils literal notranslate"><span class="pre">categories</span></code> that were perceived by the <code class="docutils literal notranslate"><span class="pre">participants</span></code> from their <code class="docutils literal notranslate"><span class="pre">brain</span> <span class="pre">activity</span> <span class="pre">patterns</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model_mlp.add(Dense(len(categories), activation = &#39;softmax&#39;))
</pre></div>
</div>
</div>
</div>
<p>To get a nice overview of our <code class="docutils literal notranslate"><span class="pre">ANN</span></code>, we can now use the <code class="docutils literal notranslate"><span class="pre">.summary()</span></code> <code class="docutils literal notranslate"><span class="pre">function</span></code>, which will provide us with the <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">type</span></code>, <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">parameters</span></code> and for each <code class="docutils literal notranslate"><span class="pre">layer</span></code>, the its <code class="docutils literal notranslate"><span class="pre">type</span></code>, <code class="docutils literal notranslate"><span class="pre">shape</span></code> and <code class="docutils literal notranslate"><span class="pre">parameters</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model_mlp.summary()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential&quot;
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>_________________________________________________________________
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Layer (type)                Output Shape              Param #   
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=================================================================
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> dense (Dense)               (None, 50)                33800     
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                                                 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> dense_1 (Dense)             (None, 30)                1530      
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                                                 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> dense_2 (Dense)             (None, 9)                 279       
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                                                 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=================================================================
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total params: 35,609
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Trainable params: 35,609
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Non-trainable params: 0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<p>With that, we already created our <code class="docutils literal notranslate"><span class="pre">MLP</span></code> <code class="docutils literal notranslate"><span class="pre">architecture</span></code>, which is now ready to be <code class="docutils literal notranslate"><span class="pre">compiled</span></code>! Within this step, we will set the <code class="docutils literal notranslate"><span class="pre">optimizer</span></code>, <code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">function</span></code> and <code class="docutils literal notranslate"><span class="pre">metric</span></code>, ie <code class="docutils literal notranslate"><span class="pre">components</span></code> that define how our <code class="docutils literal notranslate"><span class="pre">MLP</span></code> will <code class="docutils literal notranslate"><span class="pre">learn</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model_mlp.compile(optimizer = &#39;adam&#39;, loss = &#39;categorical_crossentropy&#39;, metrics = [&#39;accuracy&#39;])
</pre></div>
</div>
</div>
</div>
<p>Now it‚Äôs to <code class="docutils literal notranslate"><span class="pre">train</span></code> our <code class="docutils literal notranslate"><span class="pre">MLP</span></code>. Thus, we have to <code class="docutils literal notranslate"><span class="pre">fit</span></code> it to our <code class="docutils literal notranslate"><span class="pre">data</span></code>, specifically only the <code class="docutils literal notranslate"><span class="pre">training</span></code> <code class="docutils literal notranslate"><span class="pre">data</span></code>. Here, we are going to provide a few more <code class="docutils literal notranslate"><span class="pre">hyperparameters</span></code> that will define how our <code class="docutils literal notranslate"><span class="pre">MLP</span></code> is going to <code class="docutils literal notranslate"><span class="pre">learn</span></code>. This entails the <code class="docutils literal notranslate"><span class="pre">batch</span> <span class="pre">size</span></code>, the <code class="docutils literal notranslate"><span class="pre">epochs</span></code> and <code class="docutils literal notranslate"><span class="pre">split</span></code> of <code class="docutils literal notranslate"><span class="pre">validation</span> <span class="pre">sets</span></code>. We will assign the respective output to a variable so that we can investigate our <code class="docutils literal notranslate"><span class="pre">MLP</span></code>‚Äôs <code class="docutils literal notranslate"><span class="pre">learning</span> <span class="pre">process</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>history = model_mlp.fit(X_train, y_train, batch_size = 10,
                             epochs = 10, validation_split = 0.2)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/10
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 1/93 [..............................] - ETA: 38s - loss: 2.2402 - accuracy: 0.1000
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
36/93 [==========&gt;...................] - ETA: 0s - loss: 2.0244 - accuracy: 0.3472 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
74/93 [======================&gt;.......] - ETA: 0s - loss: 1.7611 - accuracy: 0.4297
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
93/93 [==============================] - 1s 3ms/step - loss: 1.7062 - accuracy: 0.4472 - val_loss: 1.4414 - val_accuracy: 0.5064
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 2/10
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 1/93 [..............................] - ETA: 0s - loss: 1.4880 - accuracy: 0.5000
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
41/93 [============&gt;.................] - ETA: 0s - loss: 1.1750 - accuracy: 0.6268
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
79/93 [========================&gt;.....] - ETA: 0s - loss: 1.0950 - accuracy: 0.6418
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
93/93 [==============================] - 0s 2ms/step - loss: 1.0816 - accuracy: 0.6487 - val_loss: 1.1332 - val_accuracy: 0.6352
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 3/10
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 1/93 [..............................] - ETA: 0s - loss: 0.6871 - accuracy: 0.9000
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
42/93 [============&gt;.................] - ETA: 0s - loss: 0.7616 - accuracy: 0.7524
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
79/93 [========================&gt;.....] - ETA: 0s - loss: 0.7500 - accuracy: 0.7620
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
93/93 [==============================] - 0s 2ms/step - loss: 0.7607 - accuracy: 0.7586 - val_loss: 0.9493 - val_accuracy: 0.6652
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 4/10
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 1/93 [..............................] - ETA: 0s - loss: 0.4632 - accuracy: 0.9000
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
41/93 [============&gt;.................] - ETA: 0s - loss: 0.5389 - accuracy: 0.8463
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
81/93 [=========================&gt;....] - ETA: 0s - loss: 0.5144 - accuracy: 0.8444
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
93/93 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.8448 - val_loss: 0.8824 - val_accuracy: 0.7339
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 5/10
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 1/93 [..............................] - ETA: 0s - loss: 0.3495 - accuracy: 1.0000
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
41/93 [============&gt;.................] - ETA: 0s - loss: 0.3398 - accuracy: 0.9098
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
79/93 [========================&gt;.....] - ETA: 0s - loss: 0.3255 - accuracy: 0.9114
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
93/93 [==============================] - 0s 2ms/step - loss: 0.3258 - accuracy: 0.9084 - val_loss: 0.8285 - val_accuracy: 0.7253
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 6/10
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 1/93 [..............................] - ETA: 0s - loss: 0.3895 - accuracy: 0.9000
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
40/93 [===========&gt;..................] - ETA: 0s - loss: 0.2120 - accuracy: 0.9625
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
79/93 [========================&gt;.....] - ETA: 0s - loss: 0.2197 - accuracy: 0.9582
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
93/93 [==============================] - 0s 2ms/step - loss: 0.2166 - accuracy: 0.9558 - val_loss: 0.7690 - val_accuracy: 0.7682
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 7/10
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 1/93 [..............................] - ETA: 0s - loss: 0.0574 - accuracy: 1.0000
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
41/93 [============&gt;.................] - ETA: 0s - loss: 0.1522 - accuracy: 0.9732
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
81/93 [=========================&gt;....] - ETA: 0s - loss: 0.1531 - accuracy: 0.9716
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
93/93 [==============================] - 0s 2ms/step - loss: 0.1540 - accuracy: 0.9666 - val_loss: 0.8012 - val_accuracy: 0.7339
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 8/10
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 1/93 [..............................] - ETA: 0s - loss: 0.0740 - accuracy: 1.0000
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
41/93 [============&gt;.................] - ETA: 0s - loss: 0.0968 - accuracy: 0.9805
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
80/93 [========================&gt;.....] - ETA: 0s - loss: 0.0942 - accuracy: 0.9825
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
93/93 [==============================] - 0s 2ms/step - loss: 0.0960 - accuracy: 0.9817 - val_loss: 0.7774 - val_accuracy: 0.7682
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 9/10
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 1/93 [..............................] - ETA: 0s - loss: 0.0949 - accuracy: 1.0000
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
40/93 [===========&gt;..................] - ETA: 0s - loss: 0.0533 - accuracy: 0.9950
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
80/93 [========================&gt;.....] - ETA: 0s - loss: 0.0513 - accuracy: 0.9962
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
93/93 [==============================] - 0s 2ms/step - loss: 0.0503 - accuracy: 0.9968 - val_loss: 0.7429 - val_accuracy: 0.8026
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 10/10
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 1/93 [..............................] - ETA: 0s - loss: 0.0575 - accuracy: 1.0000
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
41/93 [============&gt;.................] - ETA: 0s - loss: 0.0289 - accuracy: 1.0000
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
83/93 [=========================&gt;....] - ETA: 0s - loss: 0.0299 - accuracy: 0.9988
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
93/93 [==============================] - 0s 2ms/step - loss: 0.0306 - accuracy: 0.9989 - val_loss: 0.7360 - val_accuracy: 0.7897
</pre></div>
</div>
</div>
</div>
<p>This looks about and what we would expect the <code class="docutils literal notranslate"><span class="pre">learning</span> <span class="pre">process</span></code> to be: across <code class="docutils literal notranslate"><span class="pre">epochs</span></code>, the <code class="docutils literal notranslate"><span class="pre">loss</span></code> is decreasing and the <code class="docutils literal notranslate"><span class="pre">accuracy</span></code> is increasing.</p>
<div class="tip admonition">
<p class="admonition-title">A note regarding the learning process of our MLP</p>
<p>Comparable to its architecture, our <code class="docutils literal notranslate"><span class="pre">MLP</span></code>‚Äôs <code class="docutils literal notranslate"><span class="pre">learning</span> <span class="pre">process</span></code> is also not really what you would see on the ‚Äúreal world‚Äù. Usually, <code class="docutils literal notranslate"><span class="pre">ANN</span></code>s are <code class="docutils literal notranslate"><span class="pre">trained</span></code> way more, for longer periods of times, more <code class="docutils literal notranslate"><span class="pre">epochs</span></code> and on more <code class="docutils literal notranslate"><span class="pre">data</span></code>. However, we keep it rather short as we want to enable its application on machines with rather limited computational resources (ie your laptops or binder).</p>
</div>
<p>While this is already informative, we can also plot the <code class="docutils literal notranslate"><span class="pre">loss</span></code> and <code class="docutils literal notranslate"><span class="pre">accuracy</span></code> in the <code class="docutils literal notranslate"><span class="pre">training</span></code> and <code class="docutils literal notranslate"><span class="pre">validation</span></code> <code class="docutils literal notranslate"><span class="pre">sets</span></code> respectively. Let‚Äôs start with the <code class="docutils literal notranslate"><span class="pre">loss</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt
import seaborn as sns

plt.plot(history.history[&#39;loss&#39;], color=&#39;m&#39;)
plt.plot(history.history[&#39;val_loss&#39;], color=&#39;c&#39;)
plt.title(&#39;MLP loss&#39;)
plt.ylabel(&#39;loss&#39;)
plt.xlabel(&#39;epoch&#39;)
plt.legend([&#39;train&#39;, &#39;validation&#39;], loc = &#39;upper right&#39;)

sns.despine(offset=5)

plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/mlp_decoding_26_0.png" src="_images/mlp_decoding_26_0.png" />
</div>
</div>
<p>And now the same for the <code class="docutils literal notranslate"><span class="pre">accuracy</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt
import seaborn as sns

plt.plot(history.history[&#39;accuracy&#39;], color=&#39;m&#39;)
plt.plot(history.history[&#39;val_accuracy&#39;], color=&#39;c&#39;)
plt.title(&#39;MLP accuracy&#39;)
plt.ylabel(&#39;accuracy&#39;)
plt.xlabel(&#39;epoch&#39;)
plt.legend([&#39;train&#39;, &#39;validation&#39;], loc = &#39;upper left&#39;)

sns.despine(offset=5)

plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/mlp_decoding_28_0.png" src="_images/mlp_decoding_28_0.png" />
</div>
</div>
<div class="tip admonition">
<p class="admonition-title">How would you interpret these plots‚Ä¶</p>
<p>concerning our <code class="docutils literal notranslate"><span class="pre">MLP</span></code>‚Äôs <code class="docutils literal notranslate"><span class="pre">learning</span> <span class="pre">process</span></code>? Does it make sense? If not, how should it look like? Could you use these plots to evaluate certain aspects of the <code class="docutils literal notranslate"><span class="pre">learning</span> <span class="pre">process</span></code>, e.g. <code class="docutils literal notranslate"><span class="pre">regularization</span></code>?</p>
</div>
</div>
<div class="section" id="assessing-performance">
<h2>Assessing performance<a class="headerlink" href="#assessing-performance" title="Permalink to this headline">¬∂</a></h2>
<p>After evaluating the <code class="docutils literal notranslate"><span class="pre">training</span></code> of our <code class="docutils literal notranslate"><span class="pre">MLP</span></code>, we of course also need to evaluate its (<code class="docutils literal notranslate"><span class="pre">predictive</span></code>) <code class="docutils literal notranslate"><span class="pre">performance</span></code>. Here, this refers to the <code class="docutils literal notranslate"><span class="pre">accuracy</span></code> of our <code class="docutils literal notranslate"><span class="pre">MLP</span></code>‚Äôs outcomes, ie its <code class="docutils literal notranslate"><span class="pre">predictions</span></code>. We already saw this in the above plots and during the <code class="docutils literal notranslate"><span class="pre">training</span></code> across <code class="docutils literal notranslate"><span class="pre">epochs</span></code> but let‚Äôs check the <code class="docutils literal notranslate"><span class="pre">accuracy</span></code> of the <code class="docutils literal notranslate"><span class="pre">prediction</span></code> on the <code class="docutils literal notranslate"><span class="pre">training</span> <span class="pre">set</span></code> again:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from sklearn.metrics import classification_report
y_train_pred = model_mlp.predict(X_train)
print(classification_report(y_train.values.argmax(axis = 1), y_train_pred.argmax(axis=1)))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

           0       0.86      0.95      0.91        85
           1       0.97      0.98      0.97        88
           2       0.97      0.92      0.94        90
           3       0.97      0.96      0.97        81
           4       1.00      0.96      0.98        91
           5       0.98      0.98      0.98       471
           6       0.90      0.94      0.92        81
           7       0.98      0.96      0.97        90
           8       0.91      0.87      0.89        84

    accuracy                           0.96      1161
   macro avg       0.95      0.95      0.95      1161
weighted avg       0.96      0.96      0.96      1161
</pre></div>
</div>
</div>
</div>
<p>Why you might think: ‚ÄúOh, that‚Äôs awesome, great performance.‚Äù, such outcomes are usually perceived as dangerously high and indicate that something is off‚Ä¶</p>
<div class="tip admonition">
<p class="admonition-title">Why should a close-to-perfect performance indicate that something is wrong?</p>
<p>What do you think is the rationale to say that very high <code class="docutils literal notranslate"><span class="pre">scores</span></code> are actually ‚Äúsuspicious‚Äù and tells us that something is most likely wrong? Try thinking about the things you‚Äôve learned so far: <code class="docutils literal notranslate"><span class="pre">training</span></code>/<code class="docutils literal notranslate"><span class="pre">test</span></code>/<code class="docutils literal notranslate"><span class="pre">validation</span></code> <code class="docutils literal notranslate"><span class="pre">datasets</span></code> and their size, <code class="docutils literal notranslate"><span class="pre">models</span></code>, <code class="docutils literal notranslate"><span class="pre">predictions</span></code>, etc. .</p>
</div>
<p>Luckily, we did <code class="docutils literal notranslate"><span class="pre">split</span></code> our <code class="docutils literal notranslate"><span class="pre">dataset</span></code> into <strong>independent</strong> <code class="docutils literal notranslate"><span class="pre">training</span></code> and <code class="docutils literal notranslate"><span class="pre">test</span></code> <code class="docutils literal notranslate"><span class="pre">sets</span></code>. So, let‚Äôs check our <code class="docutils literal notranslate"><span class="pre">MLP</span></code>‚Äôs performance on the <code class="docutils literal notranslate"><span class="pre">test</span> <span class="pre">set</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y_test_pred = model_mlp.predict(X_test)
print(classification_report(y_test.values.argmax(axis = 1), y_test_pred.argmax(axis=1)))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

           0       0.70      0.83      0.76        23
           1       0.78      0.70      0.74        20
           2       0.55      0.67      0.60        18
           3       0.90      0.96      0.93        27
           4       0.88      0.88      0.88        17
           5       0.93      0.91      0.92       117
           6       0.64      0.67      0.65        27
           7       1.00      0.94      0.97        18
           8       0.67      0.50      0.57        24

    accuracy                           0.82       291
   macro avg       0.78      0.78      0.78       291
weighted avg       0.83      0.82      0.82       291
</pre></div>
</div>
</div>
</div>
<p>As you can see, the <code class="docutils literal notranslate"><span class="pre">scores</span></code>, ie <code class="docutils literal notranslate"><span class="pre">performance</span></code>, drops quite a bit. Do you know why and which you would report, e.g. in a <code class="docutils literal notranslate"><span class="pre">publication</span></code>?</p>
<p>Beside checking the overall <code class="docutils literal notranslate"><span class="pre">scores</span></code>, there are other options to further evaluate our <code class="docutils literal notranslate"><span class="pre">MLP</span></code>‚Äôs (or basically any other model‚Äôs) <code class="docutils literal notranslate"><span class="pre">performance</span></code>. One of the most commonly used ones is called <code class="docutils literal notranslate"><span class="pre">confusion</span> <span class="pre">matrix</span></code> (which you most likely have seen before in this course). A <code class="docutils literal notranslate"><span class="pre">confusion</span> <span class="pre">matrix</span></code> displays how often a given <code class="docutils literal notranslate"><span class="pre">sample</span></code> was <code class="docutils literal notranslate"><span class="pre">predicted</span></code> as a certain <code class="docutils literal notranslate"><span class="pre">label</span></code>, thus, for example, providing insights into differentiability, etc. . To implement this, we initially have to compute the <code class="docutils literal notranslate"><span class="pre">confusion</span> <span class="pre">matrix</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
from sklearn.metrics import confusion_matrix

cm_svm = confusion_matrix(y_test.values.argmax(axis = 1), y_test_pred.argmax(axis=1))
model_conf_matrix = cm_svm.astype(&#39;float&#39;) / cm_svm.sum(axis = 1)[:, np.newaxis]
</pre></div>
</div>
</div>
</div>
<p>After that, we can <code class="docutils literal notranslate"><span class="pre">plot</span></code> it for evaluation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import pandas as pd
import seaborn as sns

df_cm = pd.DataFrame(model_conf_matrix, index = categories,
                     columns = categories)

plt.figure(figsize = (10,7))
sns.heatmap(df_cm, annot = True, cmap = &#39;Blues&#39;, square = True)
plt.xticks(rotation = 45)
plt.title(&#39;MLP decoding results - confusion matrix&#39; , fontsize = 15, fontweight = &#39;bold&#39;)
plt.xlabel(&quot;true labels&quot;, fontsize = 14, fontweight = &#39;bold&#39;)
plt.ylabel(&quot;predicted labels&quot;, fontsize = 14, fontweight = &#39;bold&#39;)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/mlp_decoding_37_0.png" src="_images/mlp_decoding_37_0.png" />
</div>
</div>
<p>Based on this outcome: how would you interpret the <code class="docutils literal notranslate"><span class="pre">confusion</span> <span class="pre">matrix</span></code>? Are some <code class="docutils literal notranslate"><span class="pre">categories</span></code> better <code class="docutils literal notranslate"><span class="pre">&quot;decodable&quot;</span></code> than others? Could even make such a statement?</p>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¬∂</a></h2>
<p>With that, we already reached the end of this <code class="docutils literal notranslate"><span class="pre">tutorial</span></code> within which we talked about how to <code class="docutils literal notranslate"><span class="pre">create</span></code>, <code class="docutils literal notranslate"><span class="pre">train</span></code> and <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> a <code class="docutils literal notranslate"><span class="pre">MLP</span></code> as one possible <code class="docutils literal notranslate"><span class="pre">decoding</span> <span class="pre">model</span></code> that can be applied to <code class="docutils literal notranslate"><span class="pre">brain</span> <span class="pre">data</span></code>. As mentioned before, the <code class="docutils literal notranslate"><span class="pre">MLP</span></code> utilized here is rather simple and <code class="docutils literal notranslate"><span class="pre">models</span></code> you see (and maybe use) out in the ‚Äúreal world‚Äù will most likely be way more complex. However, their application to <code class="docutils literal notranslate"><span class="pre">brain</span> <span class="pre">data</span></code> concerning <code class="docutils literal notranslate"><span class="pre">input</span></code>, <code class="docutils literal notranslate"><span class="pre">hidden</span></code> and <code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">layers</span></code> follows the same outline.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Unfortunately, visualizing the features/transformations of an <code class="docutils literal notranslate"><span class="pre">ANN</span></code> is quite often not straightforward as it depends on the given <code class="docutils literal notranslate"><span class="pre">ANN</span></code> architecture. However, you can check this fantastic
<a class="reference external" href="https://distill.pub/2017/feature-visualization/">distill article</a> to learn more about <code class="docutils literal notranslate"><span class="pre">feature</span> <span class="pre">visualization</span></code> in <code class="docutils literal notranslate"><span class="pre">artificial</span> <span class="pre">neural</span> <span class="pre">networks</span></code>.</p>
</div>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¬∂</a></h2>
<ul class="simple">
<li><p>What is the most difficult category to decode? Why?</p></li>
<li><p>The model seemed to overfit. Try adding a <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> layer to regularize the model. You can read about dropout in keras in this <a class="reference external" href="https://towardsdatascience.com/machine-learning-part-20-dropout-keras-layers-explained-8c9f6dc4c9ab">blog post</a>.</p></li>
<li><p>Try to add layers or hidden units, and observe the impact on overfitting and training time.</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="svm_decoding.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Brain decoding with SVM</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="gcn_decoding.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Brain decoding with GCN</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>